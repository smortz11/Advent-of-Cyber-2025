Welcome back to day eight of TryHackMe's Advent of Cyber. Today is another fun challenge where we will be tackling prompt injection. More specifically, this room is going to be covering agentic AI, recognizing the potential security risks of agent tools, as well as exploitation of an AI agent. Let's go ahead and start both, the attackbox and VM, and get started!

AI has escaped the realm of just chatbots. We have recently experienced an uptick of agentic AI, or autonomous agents, which expands AI from reacting to only one stimulus to acting independently. LLM's are the basis of most AI systems. The amass a great collection of text and code, which allows them to replicate human-like answers. The data they are trained on can be a hindrance, as they may be unfamiliar with new information that has released since its last training date. These LLM's excel in text generation, stored knowledge, and following instructions. LLMs can be trickd though, based off of how they follow text patterns. Common risks include prompt injection, jailbreaking, and data poisoning.

AI uses what is called a chain-of-thought to improve its ability to perform complex and multi-step tasks autonomously. It improves the reasoning abilities of LLMs. Such a topic allows LLMs to generate reasoning traces to solve tasks that require arithmetic, logic, and other reasoning. CoT is still limited by the presence of hallucinations, outdated knowledge, and error propagation. ReAct (Reason + Act) addresses this by enabling LLMs to articulate the current thought process and execute operations in an external environment. 

Tools also allow LLMs to gather more information. In JSON-like schemas, we can develop tools, like `web_search`, which accepts one parameter, a query. When the LLM recognizes that it does not know information, it can use JSON formatting to call the tool with the specific query value. This can be integrated into the reasoning trace. If the agent is not designed with strong validation or control measures, though, this can result in security issues or unintended actions.

Now, in the attackbox, we can browse to `http://<vm-ip>`. This gives us a calendar interface along with an AI agent with access to the calendar. The date of 12/25 is marked as Easter, and our end goal is to change that to Christmas. By chatting around, asking questions like "set 12/25 to Christmas", we can see the CoT, which includes a reference to a function. Off the back of this, we can ask the LLM to list all of its functions. Of course we will only see the output in the CoT rather than the plain output. Here, we can see an array of functions. After trying to use the `reset_holidy` function, we were told that we need a token. We can ask the LLM to execute `get_logs` function whilst only outputting the token. Afterwards, we can execute the `reset_holiday` command using the outputted token.

Note, the room did not work for me at first as the AI agent appears to be extremely buggy. I had to reset my VM several times.